{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The aim is to build a part-based Active Appearance Model (AAM) in order to model the human pose. AAMs are generative statistical models of the shape and appearance of an object. Given the articulated nature of human pose, the appearance must be represented in a part-based fashion. Thus, in here, we implement the following paper:\n",
    "\n",
    "* G. Tzimiropoulos, M. Pantic. _\"Gauss-Newton Deformable Part Models for Face Alignment in-the-Wild\"_, CVPR 2014. [[pdf]](http://ibug.doc.ic.ac.uk/media/uploads/documents/tzimiro_pantic_cvpr_2014.pdf)\n",
    "\n",
    "Other relative bibliography on AAMs is:\n",
    "\n",
    "* E. Antonakos, J. Alabort-i-medina, G. Tzimiropoulos, S. Zafeiriou. _\"Feature-Based Lucas-Kanade and Active Appearance Models\"_, TIP 2015. [[pdf]](http://ibug.doc.ic.ac.uk/media/uploads/documents/antonakos2015feature.pdf)\n",
    "* I. Matthews, S. Baker. _\"Active appearance models revisited\"_, IJCV 2004. [[pdf]](http://www-cgi.cs.cmu.edu/afs/cs.cmu.edu/Web/People/efros/courses/AP06/Papers/matthews_ijcv_2004.pdf)\n",
    "* T.F. Cootes, G.J. Edwards, C.J. Taylor. _\"Active appearance models\"_, T-PAMI 2001. [[pdf]](http://www.cs.cmu.edu/~efros/courses/AP06/Papers/cootes-pami-01.pdf)\n",
    "\n",
    "The rest of the notebook is structured as follows:\n",
    "\n",
    "1. Import neccessary functionality\n",
    "2. Load training and testing data\n",
    "3. Train AAM\n",
    "4. Fit AAM\n",
    "5. Visualize results\n",
    "6. Summary and goals\n",
    "\n",
    "__NOTE__: Before starting, make should that _menpo_, _menpofit_ and _alabortijcv2015_ are updated to the latest master."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import neccessary functionality\n",
    "Let's first import all the functions that are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The purpose of this is for the visualized figures to be inline the browser\n",
    "%matplotlib inline\n",
    "\n",
    "# Import stuff from menpo\n",
    "import menpo.io as mio\n",
    "from menpo.feature import no_op, dsift, fast_dsift, double_igo\n",
    "from menpo.visualize import visualize_images, visualize_pointclouds, print_dynamic, print_progress\n",
    "from menpo.landmark import labeller, human36M_pose_32, human36M_pose_17\n",
    "\n",
    "# Import stuff from menpofit\n",
    "from menpofit.visualize import visualize_shape_model, visualize_fitting_result, plot_ced\n",
    "\n",
    "# Import stuff from alabortijcv2015 (this will be soon integrated in menpofit)\n",
    "from alabortijcv2015.aam import PartsAAMBuilder, PartsAAMFitter\n",
    "from alabortijcv2015.aam.algorithm import SIC, BSC\n",
    "from alabortijcv2015.utils import pickle_load, pickle_dump\n",
    "from alabortijcv2015.result import SerializableResult\n",
    "\n",
    "# Other stuff\n",
    "import numpy as np\n",
    "import re\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define general paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path_to_training_images = '/vol/atlas/databases/body/Human3.6M/Trainig/BySubject/S1/data/Directions_1.54138969/'\n",
    "path_to_testing_images = '/vol/atlas/databases/body/Human3.6M/Trainig/BySubject/S5/data/Directions_1.54138969/'\n",
    "save_path = '/vol/atlas/homes/nontas/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the landmarks mark-up that will be used. _Manpo_ has two labels related to the Human3.6M database:\n",
    "* _human36M_pose_32_: This includes all the initial landmarks provided with the database. However, some points are defined more than once (duplicate) and some others are not very accurately annotated.\n",
    "* _human36M_pose_17_: This is a subset of the 17 points that includes only uniquely and accurately defined points. I think it makes much more sense to use this mark-up in order to train a deformable model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "group = human36M_pose_17 #human36M_pose_32\n",
    "group_str = group.func_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load training and testing data\n",
    "Now let's load our training data. The more training data that you use, the better model you will get. From my experience on face, I believe that about 1000-2000 training images would be enough. But I'm not sure about pose. You need to **experiment** with respect to:\n",
    "* the number of training images\n",
    "* the way to subset the training images to create models for different poses.\n",
    "\n",
    "Let's first define a function for loading the data. Because the Human3.6 database in '/vol/atlas/databases/' has a specific structure, we need to define a function that will skip the '*_mask.png' images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_human36_images(path_to_training_images, crop_proportion, pattern, group, max_images=None):\n",
    "    images = []\n",
    "\n",
    "    paths_to_load = []\n",
    "    for path in mio.image_paths(path_to_training_images + '*.png'):\n",
    "        if pattern.match(path.name):\n",
    "            paths_to_load.append(path_to_training_images + path.name)\n",
    "\n",
    "    if max_images is None:\n",
    "        max_images = len(paths_to_load)\n",
    "        \n",
    "    for path in print_progress(paths_to_load[:max_images]):\n",
    "        im = mio.import_image(path)\n",
    "        im.crop_to_landmarks_proportion_inplace(crop_proportion)\n",
    "        if im.n_channels == 3:\n",
    "            im = im.as_greyscale(mode='luminosity')\n",
    "        labeller(im, 'PTS', group)\n",
    "        images.append(im)\n",
    "    \n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we need to define the options and then load the training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "crop_proportion = 0.4\n",
    "# I only need the images with pattern \"1234.png\" and not \"1234_mask.png\"\n",
    "pattern = re.compile('[0-9]{4}\\.png')\n",
    "\n",
    "training_images = load_human36_images(path_to_training_images, crop_proportion, pattern, group, max_images=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the images using the widget. By pressing the Play button we can see the actual animation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visualize_images(training_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize just the pointclouds using the respective widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_pointclouds = [im.landmarks[group_str].lms for im in training_images]\n",
    "visualize_pointclouds(training_pointclouds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same for the testing images. _Note that the testing images must not be the same as the training ones_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "crop_proportion = 0.4\n",
    "pattern = re.compile('[0-9]{4}\\.png')\n",
    "\n",
    "testing_images = load_human36_images(path_to_testing_images, crop_proportion, pattern, group, max_images=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visualize_images(testing_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train AAM\n",
    "In this step, we need to train the AAM. In general an AAM is trained in a multilevel fashion, i.e. in different resolutions. The parameters of the AAM builder are the following:\n",
    "\n",
    "* **patch_shape**: The appearance model of the AAM will be obtained by sampling appearance patches with the specified shape around each landmark.\n",
    "* **features**: The features to be used. _double_igo()_ return 4 channels, _dsift()_ 36 and _fast_dsift()_ 8.\n",
    "* **diagonal**: During building an AAM, all images are rescaled to ensure that the scale of their landmarks matches the scale of the mean shape. The scale is defined through the diagonal. Note that because the reference frame is computed from the mean landmarks, this argument also specifies the diagonal length of the reference frame (provided that features computation does not change the image size).\n",
    "* **normalize_parts**: Here we can pass in a function that will be apllied on the patches before extracting features (thus on their intensities values). The purpose of such a function would be to normalize the patch's values, e.g. by dividing with the values' norm.\n",
    "* **scales**: This argument defines the number of levels as well as their resolutions. len(scales) is the number of levels. Then each value defines the scale of each level with respect to the original resolution.\n",
    "* **max_shape_components**:  Defines the number of shape components to keep in memory. It is very important for memory efficiency.\n",
    "* **max_appearance_components**: Defines the number of appearance components to keep in memory. It is very important for memory efficiency.\n",
    "\n",
    "We select the following options and then train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patch_shape = (24, 24)\n",
    "features = fast_dsift\n",
    "diagonal = 150\n",
    "normalize_parts = no_op\n",
    "scales = (1, .5)\n",
    "max_shape_components = 25\n",
    "max_appearance_components = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aam = PartsAAMBuilder(parts_shape=patch_shape,\n",
    "                      features=features,\n",
    "                      diagonal=diagonal,\n",
    "                      normalize_parts=normalize_parts,\n",
    "                      scales=scales,\n",
    "                      max_shape_components=max_shape_components,\n",
    "                      max_appearance_components=max_appearance_components).build(training_images, group=group_str, \n",
    "                                                                                 verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can be saved as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aam_type = aam.__class__.__name__\n",
    "pickle_dump(aam, save_path + aam_type + '_' + features.__name__ + '.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then reloaded as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aam = pickle_load(save_path + 'PartsAAM_fast_dsift.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the multilevel shape model using the widgets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visualize_shape_model(aam.shape_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the eigenspectrums of the PCA models. This helps in selecting a proper number of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aam.appearance_models[1].plot_eigenvalues_ratio_widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above options play an important role to the performance of the model. We should experiment especially with **scales** and **patch_shape**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Fit AAM\n",
    "In order to fit the AAM, we first need to call the fitter. The options we need to define are:\n",
    "* **algorithm_cls**: The optimization technique to be used. After my experiments, the Simultaneous (SIC) and Bayesian (BSC) are the most suitable.\n",
    "* **n_shape**: The number of active shape components during fitting. It can be a list that specifies the components per level (low -> high).\n",
    "* **n_appearance**: The number of active appearance components during fitting. It can be a list that specifies the components per level (low -> high).\n",
    "* **sampling_mask (sampling_step)**: It defines the sampling points based on which the Jacobians will be computed. If _sampling_step == 1_, then there is no downsampling. The aim of this argument is mostly efficiency.\n",
    "* **noise_std**: The fitting procedure requires an initial estimation of the landmarks' locations. This is usually acquired by applying an object detector and then aligning the mean shape in the returned bounding box. However, since we don't have yet a human body detector, we will initialize the optimization by adding some noise to the groundtruth landmarks (annotations). This argument defines the standard deviation of this noise. If _None_, then no noise will be applied.\n",
    "* **seed**: This argument fixes the random generator engine to be used for the initialization noise. This is important since we fix the initial shapes, so we can run consistent expreriments.\n",
    "* **rotation**: Specifies whether groundtruth in-plane rotation is to be used to produce the initial shape.\n",
    "* **max_iters**: The maximum number of fitting iterations for all levels.\n",
    "* **prior**: Flag that enables/disables a regularization on the Hessian matrix (adds values on its diagonal).\n",
    "\n",
    "Let's define the options and create the fitter object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "algorithm_cls = SIC  #BSC\n",
    "n_shape = [5, 15]; \n",
    "n_appearance = [30, 50]\n",
    "sampling_step = 1\n",
    "\n",
    "noise_std = None #0.01\n",
    "seed = 2\n",
    "rotation = True\n",
    "max_iters = 50\n",
    "prior = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sampling_mask = np.require(np.zeros(patch_shape), dtype=np.bool)\n",
    "sampling_mask[::sampling_step, ::sampling_step] = True\n",
    "\n",
    "fitter = PartsAAMFitter(aam, algorithm_cls=algorithm_cls, n_shape=n_shape,\n",
    "                        n_appearance=n_appearance, sampling_mask=sampling_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now fit the model to the test images. Each fitting returns a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fitter_results = []\n",
    "\n",
    "# Fix the random seed\n",
    "np.random.seed(seed=seed)\n",
    "\n",
    "# Fit model on each test image\n",
    "for j, im in enumerate(testing_images):\n",
    "    # Get groundtruth shape\n",
    "    groundtruth_shape = im.landmarks[group_str].lms\n",
    "    # Add noise if required\n",
    "    if noise_std is not None:\n",
    "        initial_shape = fitter.perturb_shape(groundtruth_shape, noise_std=noise_std, rotation=rotation)\n",
    "    else:\n",
    "        initial_shape = groundtruth_shape\n",
    "    # Fit\n",
    "    fr = fitter.fit(im, initial_shape, gt_shape=groundtruth_shape, max_iters=max_iters, prior=prior)\n",
    "    # Append fitting result\n",
    "    fr.downscale = 0.5\n",
    "    fitter_results.append(fr)\n",
    "    # Print progress\n",
    "    print_dynamic(\"Image: {}/{}, Error: {:1.4f} -> {:1.4f}\".format(j, len(testing_images)-1, \n",
    "                                                                   fr.initial_error(), fr.final_error()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results can be saved as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = [SerializableResult('none', fr.shapes(), fr.n_iters, 'FastSIC', fr.gt_shape) \n",
    "           for fr in fitter_results]\n",
    "pickle_dump(results, save_path + aam_type + '_' + features.__name__ + '_noise' + str(noise_std) + '.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important parameters of the fitting process are **n_shape** and **n_appearance**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Visualize results\n",
    "The results can be very easily visualized using the _visualize_fitting_result()_ widget. The widget can show the initial and final shapes, the iterations as well as plot the error graphs (CED)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visualize_fitting_result(fitter_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cumulative Error Curve (CED) can also be separately plotted as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "error_type = 'me_norm' #'me_norm', or 'me' or 'rmse'\n",
    "\n",
    "initial_errors = [fr.initial_error(error_type=error_type) for fr in fitter_results]\n",
    "final_errors = [fr.final_error(error_type=error_type) for fr in fitter_results]\n",
    "\n",
    "plot_ced([initial_errors, final_errors])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also print the mean, median and standard deviation of the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"               |  mean  | median |  std  \")\n",
    "print(\"Initialization | {:1.4f} | {:1.4f} | {:1.4f}\".format(np.mean(initial_errors), \n",
    "                                                            np.median(initial_errors),\n",
    "                                                            np.std(initial_errors)))\n",
    "print(\"Fitting result | {:1.4f} | {:1.4f} | {:1.4f}\".format(np.mean(final_errors), \n",
    "                                                            np.median(final_errors),\n",
    "                                                            np.std(final_errors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Summary and goals\n",
    "The purpose of this notebook is to be used as a general guide. So the selected parameter values are not optimized at all.\n",
    "\n",
    "The most important parameters to optimize for, as mentioned before, are:\n",
    "* _patch_shape_, _scales_, _n_shape_, _n_appearance_\n",
    "\n",
    "On the other hand, parameters that also need some fixing are:\n",
    "* _diagonal_, _features_, _max_iters_, _noise_std_\n",
    "\n",
    "The most important things are:\n",
    "1. _Always visualize your model to see if it makes sense_. Specifically use the _visualize_shape_model()_ widget to see the instances that it generates. Also plot its eigenspectrum to have better understanding about the number of components that should be kept.\n",
    "2. _Always visualize your results_. The curves say nothing. You need to specifically visualize your initial shapes to see if they are too easy/hard. Of course, in a realistic scenario, the initial shapes should not be the groundtruth annotations! And then visualize the fitted shapes to see which error values actually correspond to good results. All these can be easily done using the _visualize_fitting_result()_ widget."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
